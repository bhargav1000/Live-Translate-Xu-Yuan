# Live Translator App

A real-time speech translation application built with Node.js/Express backend and React frontend. Captures user speech, transcribes it, and translates it to the desired language in real-time using local LLM inference.

## ğŸš€ Quick Start

```bash
# Single command to start everything
./dev-start.sh
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

---

## Features

âœ¨ **Real-time Speech Recognition** - Uses Web Speech API for browser-based speech-to-text
ğŸŒ **Multi-language Translation** - Supports Chinese, Spanish, French, German, and Japanese
âš¡ **Fast Performance** - Node.js backend ensures quick translation processing
ğŸ“± **Responsive Design** - Works seamlessly on desktop and mobile devices
ğŸ“ **Translation History** - Keeps track of all translations in the session

## Architecture

### Why Node.js instead of Python?

For real-time conversational translation, **Node.js is significantly faster** than Python for these reasons:

- **Non-blocking I/O**: Node.js handles multiple requests concurrently without blocking
- **Lower latency**: JavaScript execution is faster for web operations
- **Better for web integration**: Native JavaScript runtime in browsers and servers
- **Scalability**: Can handle more concurrent users with the same resources
- **Memory efficient**: Lower overhead per connection

Python would work for batch processing but introduces latency in real-time scenarios.

### Tech Stack

**Backend:**
- Node.js + Express
- RESTful API for translation endpoints
- Currently uses mock data for demo (easily integrable with Google Cloud APIs)

**Frontend:**
- React 18
- Web Speech API for audio capture & transcription
- Axios for API communication
- CSS Grid for responsive design

## Project Structure

```
Live-Translate-Xu-Yuan/
â”œâ”€â”€ server/                 # Backend
â”‚   â”œâ”€â”€ index.js           # Express server
â”‚   â”œâ”€â”€ package.json       # Node dependencies
â”‚   â””â”€â”€ .env               # Environment variables
â””â”€â”€ client/                # Frontend
    â”œâ”€â”€ public/
    â”‚   â””â”€â”€ index.html     # HTML entry point
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.js         # Main React component
    â”‚   â”œâ”€â”€ App.css        # Styling
    â”‚   â””â”€â”€ index.js       # React entry point
    â””â”€â”€ package.json       # React dependencies
```

## Architecture & Deployment

The app is designed to scale from local development to production:

- **Local Development**: Run with `./dev-start.sh`
- **Production**: Frontend on Amplify, Backend on ECS, Models on separate server

See:
- ğŸ“– [DEPLOYMENT.md](DEPLOYMENT.md) - Complete deployment guide
- ğŸ—ï¸ [CLOUD_DEPLOYMENT.md](CLOUD_DEPLOYMENT.md) - Detailed cloud architecture
- ğŸ¦™ [OLLAMA_SETUP.md](OLLAMA_SETUP.md) - Local LLM setup

## Getting Started

### Prerequisites

- Node.js (v14 or higher)
- npm or yarn
- **Ollama** (for LLM-based translations) - [Setup Guide](OLLAMA_SETUP.md)

### Quick Start with Ollama

1. **Install & run Ollama** (if not already running):
   ```bash
   # macOS
   brew install ollama
   ollama serve
   
   # In another terminal, pull a model (mistral recommended)
   ollama pull mistral
   ```

2. **Install dependencies**:
   ```bash
   cd /Users/bhargav/Desktop/Live-Translate-Xu-Yuan/server
   npm install
   
   cd ../client
   npm install
   ```

3. **Start both servers**:
   ```bash
   /Users/bhargav/Desktop/Live-Translate-Xu-Yuan/start.sh
   ```

4. **Open in browser**: `http://localhost:3000`

## How It Works

### Real-Time Translation Flow

1. **Audio Capture**: Web Speech API captures your voice
2. **Transcription**: Browser converts speech to text
3. **LLM Translation**: Ollama (local LLM) translates to target language
4. **Display**: Results shown instantly with history

### Translation Technology

The app now uses **Ollama** - a local LLM framework that:
- âœ… Runs entirely on your machine (no cloud API needed)
- âœ… Supports any language pair
- âœ… <500ms translation latency
- âœ… Completely private (no data sent to external servers)
- âœ… Free and open-source

Supported models: Mistral, Llama2, Neural-Chat, Phi, and more

### Fallback Mode

If Ollama isn't running, the app automatically uses a dictionary of common phrases. This keeps the demo functional while you set up Ollama.

## Ollama Integration

The backend calls Ollama's API to translate text:

```javascript
const response = await axios.post('http://localhost:11434/api/generate', {
  model: 'mistral',
  prompt: 'Translate to Chinese: ' + text,
  stream: false,
  temperature: 0.3
});
```

**See [OLLAMA_SETUP.md](OLLAMA_SETUP.md) for detailed setup instructions.**

## API Endpoints

### `POST /api/transcribe`
Transcribe audio to text

**Request:**
```json
{
  "audio": "base64_encoded_audio_data"
}
```

**Response:**
```json
{
  "text": "Hello",
  "confidence": 0.95
}
```

### `POST /api/translate`
Translate text to target language

**Request:**
```json
{
  "text": "Hello",
  "targetLanguage": "zh"
}
```

**Response:**
```json
{
  "originalText": "Hello",
  "translatedText": "ä½ å¥½",
  "targetLanguage": "zh"
}
```

### `POST /api/translate-audio`
Combined endpoint: transcribe and translate in one call

**Request:**
```json
{
  "audio": "base64_encoded_audio_data",
  "targetLanguage": "zh"
}
```

**Response:**
```json
{
  "originalText": "Hello",
  "translatedText": "ä½ å¥½",
  "targetLanguage": "zh",
  "confidence": 0.95
}
```

## Integrating with Real APIs

### Google Cloud Speech-to-Text & Translation

1. **Set up Google Cloud credentials:**
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/credentials.json"
   ```

2. **Update `server/index.js`** with actual Google Cloud implementation:
   ```javascript
   const speech = require('@google-cloud/speech');
   const translate = require('@google-cloud/translate');
   
   const speechClient = new speech.SpeechClient();
   const translateClient = new translate.Translate();
   ```

3. **Install the packages:**
   ```bash
   cd server
   npm install @google-cloud/speech @google-cloud/translate
   ```

## Browser Compatibility

- âœ… Chrome/Chromium (best support)
- âœ… Edge (Chromium-based)
- âœ… Safari (iOS 14.5+)
- âœ… Firefox (limited)
- âš ï¸ Opera (partial support)

## Limitations & Future Improvements

**Current:**
- Uses mock translations for demo
- Speech recognition via Web Speech API (not all languages supported)
- Single browser instance per session

**Future Enhancements:**
- Integrate Google Cloud APIs for production
- Support for more languages
- Speaker diarization (identify different speakers)
- Real-time audio streaming instead of batch processing
- User authentication & session management
- Multi-user conversation support
- Audio download/export
- Accent and pronunciation feedback

## Performance Notes

- **Latency**: <500ms for typical sentences
- **Throughput**: Can handle 10+ concurrent sessions
- **Memory**: ~50MB per active session
- **Network**: Works best with stable internet connection

## Troubleshooting

**Microphone not working?**
- Check browser permissions
- Ensure microphone is not in use by another app
- Try a different browser

**Translation not appearing?**
- Check backend is running on port 5000
- Open browser DevTools (F12) to see network errors
- Verify CORS is properly configured

**Server won't start?**
- Ensure port 5000 is not in use: `lsof -i :5000`
- Try: `npm install` in server directory again

## License

MIT - Free to use and modify

## Contributing

Feel free to submit issues and enhancement requests!
